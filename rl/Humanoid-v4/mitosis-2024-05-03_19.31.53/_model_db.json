{"_default": {"1": {"model_id": "2024-05-03_19.31.56~YOi2Kr", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_19.31.56~YOi2Kr--state_dict.pth", "model_info": {"score": 78.8152458080271, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 19:41:26.826425"}, "2": {"model_id": "2024-05-03_19.41.26~ONGcgY", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_19.41.26~ONGcgY--state_dict.pth", "model_info": {"score": 76.2372259153178, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 19:50:38.867369"}, "3": {"model_id": "2024-05-03_19.50.38~rpp1TN", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_19.50.38~rpp1TN--state_dict.pth", "model_info": {"score": 78.46640899596065, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 19:59:45.282889"}, "4": {"model_id": "2024-05-03_19.59.45~iOUmuV", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_19.59.45~iOUmuV--state_dict.pth", "model_info": {"score": 68.25622998862528, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 20:08:31.280926"}, "5": {"model_id": "2024-05-03_20.08.31~qVIc0m", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_20.08.31~qVIc0m--state_dict.pth", "model_info": {"score": 78.19743331828708, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 20:17:41.371453"}, "6": {"model_id": "2024-05-03_20.23.19~jkSCqb", "parent_model_id": "2024-05-03_20.08.31~qVIc0m", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_20.23.19~jkSCqb--state_dict.pth", "model_info": {"score": 84.10878636415866, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 20:32:46.293659"}, "7": {"model_id": "2024-05-03_20.32.46~UETzfs", "parent_model_id": "2024-05-03_20.23.19~jkSCqb", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_20.32.46~UETzfs--state_dict.pth", "model_info": {"score": 89.93549812025991, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 20:42:03.528010"}, "8": {"model_id": "2024-05-03_20.42.03~AAbsq9", "parent_model_id": "2024-05-03_20.23.19~jkSCqb", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_20.42.03~AAbsq9--state_dict.pth", "model_info": {"score": 88.97330633597048, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 20:51:29.393639"}, "9": {"model_id": "2024-05-03_20.51.29~B2PjVU", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_20.51.29~B2PjVU--state_dict.pth", "model_info": {"score": 75.36290516330575, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 21:00:17.033795"}, "10": {"model_id": "2024-05-03_21.00.17~O81lZo", "parent_model_id": "2024-05-03_19.31.56~YOi2Kr", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_21.00.17~O81lZo--state_dict.pth", "model_info": {"score": 82.53715109867274, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 21:09:57.099807"}, "11": {"model_id": "2024-05-03_21.09.57~l028kP", "parent_model_id": "2024-05-03_20.32.46~UETzfs", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_21.09.57~l028kP--state_dict.pth", "model_info": {"score": 95.91628861979648, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 21:19:13.929784"}, "12": {"model_id": "2024-05-03_21.19.14~Ha74nA", "parent_model_id": "2024-05-03_20.23.19~jkSCqb", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_21.19.14~Ha74nA--state_dict.pth", "model_info": {"score": 92.18661136869525, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 21:28:45.492484"}, "13": {"model_id": "2024-05-03_21.30.22~qZu9T3", "parent_model_id": "2024-05-03_21.19.14~Ha74nA", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_21.30.22~qZu9T3--state_dict.pth", "model_info": {"score": 97.63969656648499, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 21:39:49.532275"}, "14": {"model_id": "2024-05-03_21.43.20~RfUGC7", "parent_model_id": "2024-05-03_21.09.57~l028kP", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_21.43.20~RfUGC7--state_dict.pth", "model_info": {"score": 109.03835898843656, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 21:52:38.070436"}, "15": {"model_id": "2024-05-03_21.52.38~IWwG2v", "parent_model_id": "2024-05-03_20.42.03~AAbsq9", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_21.52.38~IWwG2v--state_dict.pth", "model_info": {"score": 91.26367081250771, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:01:54.583489"}, "16": {"model_id": "2024-05-03_22.01.54~ACeu6l", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.01.54~ACeu6l--state_dict.pth", "model_info": {"score": 88.28104158012763, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:10:48.546196"}, "17": {"model_id": "2024-05-03_22.10.48~pYUxFh", "parent_model_id": "2024-05-03_20.08.31~qVIc0m", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.10.48~pYUxFh--state_dict.pth", "model_info": {"score": 92.16406853117707, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:20:12.807353"}, "18": {"model_id": "2024-05-03_22.20.12~h4k0Gh", "parent_model_id": "2024-05-03_21.43.20~RfUGC7", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.20.12~h4k0Gh--state_dict.pth", "model_info": {"score": 108.05864939800809, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:29:29.042929"}, "19": {"model_id": "2024-05-03_22.29.29~Rs0F3r", "parent_model_id": "2024-05-03_20.23.19~jkSCqb", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.29.29~Rs0F3r--state_dict.pth", "model_info": {"score": 91.502497610179, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:38:52.387697"}, "20": {"model_id": "2024-05-03_22.38.52~Xu8Xy5", "parent_model_id": "2024-05-03_21.19.14~Ha74nA", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.38.52~Xu8Xy5--state_dict.pth", "model_info": {"score": 98.11174871036432, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:48:06.564710"}, "21": {"model_id": "2024-05-03_22.48.06~ZTYgTN", "parent_model_id": "2024-05-03_21.43.20~RfUGC7", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.48.06~ZTYgTN--state_dict.pth", "model_info": {"score": 119.15549908168623, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 22:57:17.481189"}, "22": {"model_id": "2024-05-03_22.57.17~SjuQMY", "parent_model_id": "2024-05-03_20.42.03~AAbsq9", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_22.57.17~SjuQMY--state_dict.pth", "model_info": {"score": 94.81828243933919, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 23:06:42.694719"}, "23": {"model_id": "2024-05-03_23.06.42~60oZB5", "parent_model_id": "2024-05-03_20.42.03~AAbsq9", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_23.06.42~60oZB5--state_dict.pth", "model_info": {"score": 89.70892788437499, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 23:16:07.003851"}, "24": {"model_id": "2024-05-03_23.16.07~Ntmb7N", "parent_model_id": "2024-05-03_22.20.12~h4k0Gh", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_23.16.07~Ntmb7N--state_dict.pth", "model_info": {"score": 119.19286514830239, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 23:25:22.949334"}, "25": {"model_id": "2024-05-03_23.25.23~F54ySp", "parent_model_id": "2024-05-03_22.48.06~ZTYgTN", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_23.25.23~F54ySp--state_dict.pth", "model_info": {"score": 128.31969649510214, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 23:34:33.369021"}, "26": {"model_id": "2024-05-03_23.34.33~z9rEfe", "parent_model_id": "2024-05-03_23.16.07~Ntmb7N", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_23.34.33~z9rEfe--state_dict.pth", "model_info": {"score": 125.2645564802101, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 23:43:45.609218"}, "27": {"model_id": "2024-05-03_23.43.45~3uRz5g", "parent_model_id": "2024-05-03_23.16.07~Ntmb7N", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_23.43.45~3uRz5g--state_dict.pth", "model_info": {"score": 131.10465327531443, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-03 23:52:58.131142"}, "28": {"model_id": "2024-05-03_23.52.58~bSXC4u", "parent_model_id": "2024-05-03_21.43.20~RfUGC7", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-03_23.52.58~bSXC4u--state_dict.pth", "model_info": {"score": 117.92829495421329, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 00:02:10.277534"}, "29": {"model_id": "2024-05-04_00.02.10~DQW8KB", "parent_model_id": "2024-05-03_23.52.58~bSXC4u", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_00.02.10~DQW8KB--state_dict.pth", "model_info": {"score": 106.8704113551181, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 00:11:21.648921"}, "30": {"model_id": "2024-05-04_00.11.21~MQ4D7Q", "parent_model_id": "2024-05-03_23.25.23~F54ySp", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_00.11.21~MQ4D7Q--state_dict.pth", "model_info": {"score": 113.21745606562858, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 00:20:23.186834"}, "31": {"model_id": "2024-05-04_00.20.23~MV6RVg", "parent_model_id": "2024-05-03_23.43.45~3uRz5g", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_00.20.23~MV6RVg--state_dict.pth", "model_info": {"score": 146.80993738690782, "steps_trained": 900000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 00:29:39.305714"}, "32": {"model_id": "2024-05-04_18.03.48~Ql7TTA", "parent_model_id": "2024-05-03_22.10.48~pYUxFh", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_18.03.48~Ql7TTA--state_dict.pth", "model_info": {"score": 94.83323261129543, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 18:13:24.469598"}, "33": {"model_id": "2024-05-04_18.13.24~WUZKMx", "parent_model_id": "2024-05-04_18.03.48~Ql7TTA", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_18.13.24~WUZKMx--state_dict.pth", "model_info": {"score": 92.39629858029366, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 18:22:47.014995"}, "34": {"model_id": "2024-05-04_18.22.47~VSYMLU", "parent_model_id": "2024-05-04_18.13.24~WUZKMx", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_18.22.47~VSYMLU--state_dict.pth", "model_info": {"score": 98.31250614671572, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 18:32:05.700517"}, "35": {"model_id": "2024-05-04_18.32.05~vcAUuK", "parent_model_id": "2024-05-04_18.22.47~VSYMLU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_18.32.05~vcAUuK--state_dict.pth", "model_info": {"score": 99.5329719909838, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 18:41:28.131563"}, "36": {"model_id": "2024-05-04_18.41.28~yKmJMk", "parent_model_id": "2024-05-04_18.32.05~vcAUuK", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_18.41.28~yKmJMk--state_dict.pth", "model_info": {"score": 102.76795058377036, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 18:50:58.144277"}, "37": {"model_id": "2024-05-04_18.50.58~LXSqBi", "parent_model_id": "2024-05-04_18.41.28~yKmJMk", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_18.50.58~LXSqBi--state_dict.pth", "model_info": {"score": 106.9669706809428, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 19:00:29.709696"}, "38": {"model_id": "2024-05-04_19.00.29~pquFIW", "parent_model_id": "2024-05-04_18.50.58~LXSqBi", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-03_19.31.53\\2024-05-04_19.00.29~pquFIW--state_dict.pth", "model_info": {"score": 105.08517266844875, "steps_trained": 900000, "init_policy_source_code": "def init_policy():\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n            \n            in_size = 376\n            shared_out_sizes = []\n            actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256, 17]\n            critic_out_sizes = [512, 512, 256, 256, 256, 1]\n            \n            hidden_activation_function = nn.ELU()\n            actor_out_activation_function = nn.Tanh()\n            critic_out_activation_function = nn.Identity()\n            \n            self.has_shared = len(shared_out_sizes) > 0\n            \n            if self.has_shared:\n                self.shared = SeqNet.from_layer_provider(\n                    layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                        nn.Linear(in_features, out_features),\n                        hidden_activation_function\n                    ),\n                    in_size=in_size,\n                    out_sizes=shared_out_sizes\n                )\n            else:\n                self.shared = TorchNet(nn.Identity(), in_shape=TensorShape(features=in_size), out_shape=TensorShape(features=in_size))\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    actor_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    critic_out_activation_function if is_last_layer else hidden_activation_function\n                ),\n                in_size=self.shared.out_shape.get_definite_features(),\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            if self.has_shared:\n                shared_out = self.shared(x)\n            else:\n                shared_out = x\n\n            return self.actor(shared_out), self.critic(shared_out)\n\n    return ActorCriticPolicy(A2CNetwork(), lambda action_logits: dist.Normal(loc=action_logits, scale=policy_action_std))\n", "wrap_env_source_code": "def wrap_env(_env: VectorEnv):\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward) \n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n    _env = ClipAction(_env)\n    return _env\n"}, "last_update_time": "2024-05-04 19:10:00.159666"}}}
{"_default": {"1": {"model_id": "2024-05-17_19.33.58~rT8tlm", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.33.58~rT8tlm--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.33.58~rT8tlm", "parent_policy_id": null, "score": 71.56049592136947, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:39:44.526346"}, "2": {"model_id": "2024-05-17_19.33.58~0FHGB1", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.33.58~0FHGB1--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.33.58~0FHGB1", "parent_policy_id": null, "score": 80.00540442435103, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:39:45.957392"}, "3": {"model_id": "2024-05-17_19.33.58~NmSSSD", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.33.58~NmSSSD--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.33.58~NmSSSD", "parent_policy_id": null, "score": 74.20550665198616, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:39:47.243704"}, "4": {"model_id": "2024-05-17_19.39.44~O8JCgN", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.39.44~O8JCgN--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.39.44~O8JCgN", "parent_policy_id": null, "score": 71.58384311934293, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:45:27.748624"}, "5": {"model_id": "2024-05-17_19.39.47~0htDYq", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.39.47~0htDYq--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.39.47~0htDYq", "parent_policy_id": null, "score": 74.82484539326961, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:45:29.127936"}, "6": {"model_id": "2024-05-17_19.39.46~8CDohK", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.39.46~8CDohK--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.39.46~8CDohK", "parent_policy_id": null, "score": 76.82489172013516, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:45:31.091327"}, "7": {"model_id": "2024-05-17_19.45.29~mJ8pek", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.45.29~mJ8pek--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.45.29~mJ8pek", "parent_policy_id": null, "score": 66.98652279260317, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:51:05.338929"}, "8": {"model_id": "2024-05-17_19.45.27~e4FAZe", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.45.27~e4FAZe--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.45.27~e4FAZe", "parent_policy_id": null, "score": 74.40480520138703, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:51:11.088941"}, "9": {"model_id": "2024-05-17_19.45.31~AgRbb7", "parent_model_id": "2024-05-17_19.39.46~8CDohK", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.45.31~AgRbb7--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.45.31~AgRbb7", "parent_policy_id": "2024-05-17_19.39.46~8CDohK", "score": 75.27601540057474, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:51:16.715760"}, "10": {"model_id": "2024-05-17_19.51.05~MQ2dXR", "parent_model_id": "2024-05-17_19.39.46~8CDohK", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.51.05~MQ2dXR--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.51.05~MQ2dXR", "parent_policy_id": "2024-05-17_19.39.46~8CDohK", "score": 80.25792968267449, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:56:50.751244"}, "11": {"model_id": "2024-05-17_19.51.11~0SrmlW", "parent_model_id": "2024-05-17_19.33.58~0FHGB1", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.51.11~0SrmlW--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.51.11~0SrmlW", "parent_policy_id": "2024-05-17_19.33.58~0FHGB1", "score": 81.19961117257196, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:56:57.576455"}, "12": {"model_id": "2024-05-17_19.51.16~kjnJNU", "parent_model_id": "2024-05-17_19.33.58~0FHGB1", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.51.16~kjnJNU--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.51.16~kjnJNU", "parent_policy_id": "2024-05-17_19.33.58~0FHGB1", "score": 84.5572326528467, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 19:57:01.915799"}, "13": {"model_id": "2024-05-17_19.56.50~YutIIp", "parent_model_id": "2024-05-17_19.51.05~MQ2dXR", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.56.50~YutIIp--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.56.50~YutIIp", "parent_policy_id": "2024-05-17_19.51.05~MQ2dXR", "score": 78.76088786622358, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:02:36.760183"}, "14": {"model_id": "2024-05-17_19.56.57~CFrHxn", "parent_model_id": "2024-05-17_19.33.58~0FHGB1", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.56.57~CFrHxn--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.56.57~CFrHxn", "parent_policy_id": "2024-05-17_19.33.58~0FHGB1", "score": 78.16425297751987, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:02:38.699791"}, "15": {"model_id": "2024-05-17_19.57.02~srx8CP", "parent_model_id": "2024-05-17_19.39.47~0htDYq", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_19.57.02~srx8CP--state_dict.pth", "model_info": {"policy_id": "2024-05-17_19.57.02~srx8CP", "parent_policy_id": "2024-05-17_19.39.47~0htDYq", "score": 76.35173559563634, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:02:41.943801"}, "16": {"model_id": "2024-05-17_20.02.38~2PthbR", "parent_model_id": "2024-05-17_19.51.16~kjnJNU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.02.38~2PthbR--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.02.38~2PthbR", "parent_policy_id": "2024-05-17_19.51.16~kjnJNU", "score": 82.52621578847439, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:08:20.272500"}, "17": {"model_id": "2024-05-17_20.02.36~hSYXsA", "parent_model_id": "2024-05-17_19.56.50~YutIIp", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.02.36~hSYXsA--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.02.36~hSYXsA", "parent_policy_id": "2024-05-17_19.56.50~YutIIp", "score": 74.82614803732801, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:08:23.841308"}, "18": {"model_id": "2024-05-17_20.02.42~MD4syX", "parent_model_id": "2024-05-17_19.51.16~kjnJNU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.02.42~MD4syX--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.02.42~MD4syX", "parent_policy_id": "2024-05-17_19.51.16~kjnJNU", "score": 88.07663139436045, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:08:28.631747"}, "19": {"model_id": "2024-05-17_20.08.20~aVzfnk", "parent_model_id": "2024-05-17_19.51.16~kjnJNU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.08.20~aVzfnk--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.08.20~aVzfnk", "parent_policy_id": "2024-05-17_19.51.16~kjnJNU", "score": 78.90160528332684, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:14:04.435771"}, "20": {"model_id": "2024-05-17_20.08.23~X2ErTh", "parent_model_id": "2024-05-17_19.51.11~0SrmlW", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.08.23~X2ErTh--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.08.23~X2ErTh", "parent_policy_id": "2024-05-17_19.51.11~0SrmlW", "score": 84.98394206360453, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:14:06.767045"}, "21": {"model_id": "2024-05-17_20.08.28~ieMudN", "parent_model_id": "2024-05-17_19.51.16~kjnJNU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.08.28~ieMudN--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.08.28~ieMudN", "parent_policy_id": "2024-05-17_19.51.16~kjnJNU", "score": 79.41839029459638, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:14:13.448618"}, "22": {"model_id": "2024-05-17_20.14.04~2lkIB3", "parent_model_id": "2024-05-17_19.56.57~CFrHxn", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.14.04~2lkIB3--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.14.04~2lkIB3", "parent_policy_id": "2024-05-17_19.56.57~CFrHxn", "score": 76.77758463168337, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:19:45.955331"}, "23": {"model_id": "2024-05-17_20.14.06~KliK2P", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.14.06~KliK2P--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.14.06~KliK2P", "parent_policy_id": null, "score": 79.9409444707018, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:19:46.063355"}, "24": {"model_id": "2024-05-17_20.14.13~VlhvIe", "parent_model_id": "2024-05-17_20.02.38~2PthbR", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.14.13~VlhvIe--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.14.13~VlhvIe", "parent_policy_id": "2024-05-17_20.02.38~2PthbR", "score": 89.47912467400644, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:19:55.201743"}, "25": {"model_id": "2024-05-17_20.19.46~SFyPOd", "parent_model_id": "2024-05-17_20.14.04~2lkIB3", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.19.46~SFyPOd--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.19.46~SFyPOd", "parent_policy_id": "2024-05-17_20.14.04~2lkIB3", "score": 72.44220011625963, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:25:25.600100"}, "26": {"model_id": "2024-05-17_20.19.46~gFvca8", "parent_model_id": "2024-05-17_20.02.42~MD4syX", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.19.46~gFvca8--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.19.46~gFvca8", "parent_policy_id": "2024-05-17_20.02.42~MD4syX", "score": 93.47893225424949, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:25:30.471437"}, "27": {"model_id": "2024-05-17_20.19.55~rBBJl3", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.19.55~rBBJl3--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.19.55~rBBJl3", "parent_policy_id": null, "score": 75.52652937898048, "steps_trained": 100000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:25:38.763156"}, "28": {"model_id": "2024-05-17_20.25.25~NwZaQO", "parent_model_id": "2024-05-17_20.02.42~MD4syX", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.25.25~NwZaQO--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.25.25~NwZaQO", "parent_policy_id": "2024-05-17_20.02.42~MD4syX", "score": 88.0662443470989, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:31:11.736513"}, "29": {"model_id": "2024-05-17_20.25.30~9jchAh", "parent_model_id": "2024-05-17_20.19.46~gFvca8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.25.30~9jchAh--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.25.30~9jchAh", "parent_policy_id": "2024-05-17_20.19.46~gFvca8", "score": 88.51181682195198, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:31:16.548034"}, "30": {"model_id": "2024-05-17_20.25.38~BfVdcQ", "parent_model_id": "2024-05-17_20.19.46~gFvca8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.25.38~BfVdcQ--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.25.38~BfVdcQ", "parent_policy_id": "2024-05-17_20.19.46~gFvca8", "score": 97.71962848518264, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:31:24.968541"}, "31": {"model_id": "2024-05-17_20.31.16~A2f1Rt", "parent_model_id": "2024-05-17_20.14.06~KliK2P", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.31.16~A2f1Rt--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.31.16~A2f1Rt", "parent_policy_id": "2024-05-17_20.14.06~KliK2P", "score": 77.14085668989637, "steps_trained": 200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:36:53.154396"}, "32": {"model_id": "2024-05-17_20.31.11~058mhZ", "parent_model_id": "2024-05-17_20.19.46~gFvca8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.31.11~058mhZ--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.31.11~058mhZ", "parent_policy_id": "2024-05-17_20.19.46~gFvca8", "score": 99.27101340958254, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:36:54.184785"}, "33": {"model_id": "2024-05-17_20.31.25~hCa8RU", "parent_model_id": "2024-05-17_20.25.38~BfVdcQ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.31.25~hCa8RU--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.31.25~hCa8RU", "parent_policy_id": "2024-05-17_20.25.38~BfVdcQ", "score": 93.67678270969591, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:37:11.046516"}, "34": {"model_id": "2024-05-17_20.36.53~9C3zQp", "parent_model_id": "2024-05-17_20.02.38~2PthbR", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.36.53~9C3zQp--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.36.53~9C3zQp", "parent_policy_id": "2024-05-17_20.02.38~2PthbR", "score": 82.94569733284607, "steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:42:33.438686"}, "35": {"model_id": "2024-05-17_20.36.54~Kr93Q8", "parent_model_id": "2024-05-17_20.25.38~BfVdcQ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.36.54~Kr93Q8--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.36.54~Kr93Q8", "parent_policy_id": "2024-05-17_20.25.38~BfVdcQ", "score": 100.61874424046991, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:42:40.772490"}, "36": {"model_id": "2024-05-17_20.37.11~BjDzB6", "parent_model_id": "2024-05-17_19.45.31~AgRbb7", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.37.11~BjDzB6--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.37.11~BjDzB6", "parent_policy_id": "2024-05-17_19.45.31~AgRbb7", "score": 78.71498513499029, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:42:59.144359"}, "37": {"model_id": "2024-05-17_20.42.33~KkxeD7", "parent_model_id": "2024-05-17_20.31.25~hCa8RU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.42.33~KkxeD7--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.42.33~KkxeD7", "parent_policy_id": "2024-05-17_20.31.25~hCa8RU", "score": 97.12015272824127, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:48:21.755613"}, "38": {"model_id": "2024-05-17_20.42.40~iOs1k9", "parent_model_id": "2024-05-17_20.19.46~gFvca8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.42.40~iOs1k9--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.42.40~iOs1k9", "parent_policy_id": "2024-05-17_20.19.46~gFvca8", "score": 97.7133293168674, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:48:21.908648"}, "39": {"model_id": "2024-05-17_20.42.59~uA78OJ", "parent_model_id": "2024-05-17_20.36.54~Kr93Q8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.42.59~uA78OJ--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.42.59~uA78OJ", "parent_policy_id": "2024-05-17_20.36.54~Kr93Q8", "score": 94.8786482633588, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:48:45.567372"}, "40": {"model_id": "2024-05-17_20.48.22~dyjqw8", "parent_model_id": "2024-05-17_20.31.11~058mhZ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.48.22~dyjqw8--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.48.22~dyjqw8", "parent_policy_id": "2024-05-17_20.31.11~058mhZ", "score": 97.67668054001592, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:54:06.030104"}, "41": {"model_id": "2024-05-17_20.48.21~21Aqka", "parent_model_id": "2024-05-17_20.42.33~KkxeD7", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.48.21~21Aqka--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.48.21~21Aqka", "parent_policy_id": "2024-05-17_20.42.33~KkxeD7", "score": 94.34862005093285, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:54:09.533508"}, "42": {"model_id": "2024-05-17_20.48.45~NENbDM", "parent_model_id": "2024-05-17_20.31.11~058mhZ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.48.45~NENbDM--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.48.45~NENbDM", "parent_policy_id": "2024-05-17_20.31.11~058mhZ", "score": 101.18813503081171, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:54:29.463147"}, "43": {"model_id": "2024-05-17_20.54.06~51T7FD", "parent_model_id": "2024-05-17_20.31.11~058mhZ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.54.06~51T7FD--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.54.06~51T7FD", "parent_policy_id": "2024-05-17_20.31.11~058mhZ", "score": 96.14182675793386, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:59:50.880420"}, "44": {"model_id": "2024-05-17_20.54.09~gYiNkY", "parent_model_id": "2024-05-17_20.36.54~Kr93Q8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.54.09~gYiNkY--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.54.09~gYiNkY", "parent_policy_id": "2024-05-17_20.36.54~Kr93Q8", "score": 94.33846713617788, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 20:59:54.061883"}, "45": {"model_id": "2024-05-17_20.54.29~yll2sB", "parent_model_id": "2024-05-17_20.42.40~iOs1k9", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.54.29~yll2sB--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.54.29~yll2sB", "parent_policy_id": "2024-05-17_20.42.40~iOs1k9", "score": 101.04473940829635, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:00:10.058645"}, "46": {"model_id": "2024-05-17_20.59.50~aJRFlO", "parent_model_id": "2024-05-17_20.19.46~gFvca8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.59.50~aJRFlO--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.59.50~aJRFlO", "parent_policy_id": "2024-05-17_20.19.46~gFvca8", "score": 96.40627898229661, "steps_trained": 500000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:05:33.760523"}, "47": {"model_id": "2024-05-17_20.59.54~jsHmfE", "parent_model_id": "2024-05-17_20.36.54~Kr93Q8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_20.59.54~jsHmfE--state_dict.pth", "model_info": {"policy_id": "2024-05-17_20.59.54~jsHmfE", "parent_policy_id": "2024-05-17_20.36.54~Kr93Q8", "score": 95.54820118418735, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:05:40.545080"}, "48": {"model_id": "2024-05-17_21.00.10~hUJ08L", "parent_model_id": "2024-05-17_20.31.11~058mhZ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.00.10~hUJ08L--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.00.10~hUJ08L", "parent_policy_id": "2024-05-17_20.31.11~058mhZ", "score": 92.80187849595998, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:05:53.229267"}, "49": {"model_id": "2024-05-17_21.05.33~cGGFy8", "parent_model_id": "2024-05-17_20.42.40~iOs1k9", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.05.33~cGGFy8--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.05.33~cGGFy8", "parent_policy_id": "2024-05-17_20.42.40~iOs1k9", "score": 102.03894946059958, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:11:13.934822"}, "50": {"model_id": "2024-05-17_21.05.40~q0NwNN", "parent_model_id": "2024-05-17_20.54.06~51T7FD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.05.40~q0NwNN--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.05.40~q0NwNN", "parent_policy_id": "2024-05-17_20.54.06~51T7FD", "score": 100.56526969848689, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:11:26.288504"}, "51": {"model_id": "2024-05-17_21.05.53~Rf0fyj", "parent_model_id": "2024-05-17_20.54.09~gYiNkY", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.05.53~Rf0fyj--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.05.53~Rf0fyj", "parent_policy_id": "2024-05-17_20.54.09~gYiNkY", "score": 100.00764595624753, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:11:38.658978"}, "52": {"model_id": "2024-05-17_21.11.14~6cLsqH", "parent_model_id": "2024-05-17_20.54.29~yll2sB", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.11.14~6cLsqH--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.11.14~6cLsqH", "parent_policy_id": "2024-05-17_20.54.29~yll2sB", "score": 105.14707971805728, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:16:56.561271"}, "53": {"model_id": "2024-05-17_21.11.26~3cGRI6", "parent_model_id": "2024-05-17_20.48.45~NENbDM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.11.26~3cGRI6--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.11.26~3cGRI6", "parent_policy_id": "2024-05-17_20.48.45~NENbDM", "score": 99.25044961236215, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:17:05.407551"}, "54": {"model_id": "2024-05-17_21.11.38~ocQ6Y3", "parent_model_id": "2024-05-17_20.54.06~51T7FD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.11.38~ocQ6Y3--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.11.38~ocQ6Y3", "parent_policy_id": "2024-05-17_20.54.06~51T7FD", "score": 96.41896990552974, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:17:24.996302"}, "55": {"model_id": "2024-05-17_21.16.56~YCJZXU", "parent_model_id": "2024-05-17_20.48.45~NENbDM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.16.56~YCJZXU--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.16.56~YCJZXU", "parent_policy_id": "2024-05-17_20.48.45~NENbDM", "score": 99.73533111457684, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:22:39.490111"}, "56": {"model_id": "2024-05-17_21.17.05~BcXArU", "parent_model_id": "2024-05-17_20.25.38~BfVdcQ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.17.05~BcXArU--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.17.05~BcXArU", "parent_policy_id": "2024-05-17_20.25.38~BfVdcQ", "score": 95.31882857842473, "steps_trained": 600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:22:51.288898"}, "57": {"model_id": "2024-05-17_21.17.25~mbTCLN", "parent_model_id": "2024-05-17_20.59.54~jsHmfE", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.17.25~mbTCLN--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.17.25~mbTCLN", "parent_policy_id": "2024-05-17_20.59.54~jsHmfE", "score": 98.322112251971, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:23:11.499308"}, "58": {"model_id": "2024-05-17_21.22.39~bjlIq9", "parent_model_id": "2024-05-17_20.48.22~dyjqw8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.22.39~bjlIq9--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.22.39~bjlIq9", "parent_policy_id": "2024-05-17_20.48.22~dyjqw8", "score": 96.02526134560257, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:28:22.190204"}, "59": {"model_id": "2024-05-17_21.22.51~hRW60a", "parent_model_id": "2024-05-17_20.36.54~Kr93Q8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.22.51~hRW60a--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.22.51~hRW60a", "parent_policy_id": "2024-05-17_20.36.54~Kr93Q8", "score": 98.20237579929706, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:28:35.466813"}, "60": {"model_id": "2024-05-17_21.23.11~6ady6I", "parent_model_id": "2024-05-17_20.54.29~yll2sB", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.23.11~6ady6I--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.23.11~6ady6I", "parent_policy_id": "2024-05-17_20.54.29~yll2sB", "score": 90.69292630138105, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:28:50.060538"}, "61": {"model_id": "2024-05-17_21.28.22~ylAYqt", "parent_model_id": "2024-05-17_20.36.54~Kr93Q8", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.28.22~ylAYqt--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.28.22~ylAYqt", "parent_policy_id": "2024-05-17_20.36.54~Kr93Q8", "score": 97.89262031261754, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:34:07.505972"}, "62": {"model_id": "2024-05-17_21.28.35~DIgVb1", "parent_model_id": "2024-05-17_20.48.45~NENbDM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.28.35~DIgVb1--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.28.35~DIgVb1", "parent_policy_id": "2024-05-17_20.48.45~NENbDM", "score": 102.87829855187204, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:34:16.706758"}, "63": {"model_id": "2024-05-17_21.28.50~8jHPLD", "parent_model_id": "2024-05-17_21.11.26~3cGRI6", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.28.50~8jHPLD--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.28.50~8jHPLD", "parent_policy_id": "2024-05-17_21.11.26~3cGRI6", "score": 104.04868319820264, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:34:28.335330"}, "64": {"model_id": "2024-05-17_21.34.07~C3AoxQ", "parent_model_id": "2024-05-17_20.54.06~51T7FD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.34.07~C3AoxQ--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.34.07~C3AoxQ", "parent_policy_id": "2024-05-17_20.54.06~51T7FD", "score": 96.02234413785018, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:39:52.788433"}, "65": {"model_id": "2024-05-17_21.34.16~xKGjAn", "parent_model_id": "2024-05-17_19.45.31~AgRbb7", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.34.16~xKGjAn--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.34.16~xKGjAn", "parent_policy_id": "2024-05-17_19.45.31~AgRbb7", "score": 76.96717391381438, "steps_trained": 300000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:40:05.886073"}, "66": {"model_id": "2024-05-17_21.34.28~jZ6KhU", "parent_model_id": "2024-05-17_20.54.29~yll2sB", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.34.28~jZ6KhU--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.34.28~jZ6KhU", "parent_policy_id": "2024-05-17_20.54.29~yll2sB", "score": 99.21420403964689, "steps_trained": 700000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:40:12.707446"}, "67": {"model_id": "2024-05-17_21.39.53~IvB2DN", "parent_model_id": "2024-05-17_21.16.56~YCJZXU", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.39.53~IvB2DN--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.39.53~IvB2DN", "parent_policy_id": "2024-05-17_21.16.56~YCJZXU", "score": 101.96326851246452, "steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:45:41.302098"}, "68": {"model_id": "2024-05-17_21.40.06~tjQir1", "parent_model_id": "2024-05-17_20.48.21~21Aqka", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.40.06~tjQir1--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.40.06~tjQir1", "parent_policy_id": "2024-05-17_20.48.21~21Aqka", "score": 96.19321477459144, "steps_trained": 900000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:45:55.636610"}, "69": {"model_id": "2024-05-17_21.40.12~G5Ibfx", "parent_model_id": "2024-05-17_21.28.50~8jHPLD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-17_19.33.57\\2024-05-17_21.40.12~G5Ibfx--state_dict.pth", "model_info": {"policy_id": "2024-05-17_21.40.12~G5Ibfx", "parent_policy_id": "2024-05-17_21.28.50~8jHPLD", "score": 103.58404620930162, "steps_trained": 900000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-17 21:45:56.714854"}}}
{"_default": {"1": {"model_id": "2024-05-18_02.04.36~E0JgPM", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.04.36~E0JgPM--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.04.36~E0JgPM", "parent_policy_id": null, "score": 78.06068098447211, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:10:22.190731"}, "2": {"model_id": "2024-05-18_02.04.36~kjFWje", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.04.36~kjFWje--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.04.36~kjFWje", "parent_policy_id": null, "score": 77.20620843918104, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:10:24.037144"}, "3": {"model_id": "2024-05-18_02.04.36~iJLFQm", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.04.36~iJLFQm--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.04.36~iJLFQm", "parent_policy_id": null, "score": 78.4314532256695, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:10:24.577265"}, "4": {"model_id": "2024-05-18_02.10.24~xp4LTS", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.10.24~xp4LTS--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.10.24~xp4LTS", "parent_policy_id": null, "score": 63.77578908558362, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:16:07.491434"}, "5": {"model_id": "2024-05-18_02.10.24~e7yMFY", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.10.24~e7yMFY--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.10.24~e7yMFY", "parent_policy_id": null, "score": 66.5537579701439, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:16:08.426542"}, "6": {"model_id": "2024-05-18_02.10.22~OUv5D1", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.10.22~OUv5D1--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.10.22~OUv5D1", "parent_policy_id": null, "score": 75.02290424779484, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:16:10.984114"}, "7": {"model_id": "2024-05-18_02.16.07~cn1Ey6", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.16.07~cn1Ey6--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.16.07~cn1Ey6", "parent_policy_id": null, "score": 63.40152300172288, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:21:43.870099"}, "8": {"model_id": "2024-05-18_02.16.08~2npBpF", "parent_model_id": "2024-05-18_02.04.36~iJLFQm", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.16.08~2npBpF--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.16.08~2npBpF", "parent_policy_id": "2024-05-18_02.04.36~iJLFQm", "score": 74.35347522191675, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:21:54.752523"}, "9": {"model_id": "2024-05-18_02.16.11~cjlxC6", "parent_model_id": "2024-05-18_02.04.36~E0JgPM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.16.11~cjlxC6--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.16.11~cjlxC6", "parent_policy_id": "2024-05-18_02.04.36~E0JgPM", "score": 75.5813138392815, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:21:57.690834"}, "10": {"model_id": "2024-05-18_02.21.43~UfLGtl", "parent_model_id": "2024-05-18_02.04.36~E0JgPM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.21.43~UfLGtl--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.21.43~UfLGtl", "parent_policy_id": "2024-05-18_02.04.36~E0JgPM", "score": 72.57005146790242, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:27:28.159464"}, "11": {"model_id": "2024-05-18_02.21.54~4QubTI", "parent_model_id": "2024-05-18_02.04.36~E0JgPM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.21.54~4QubTI--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.21.54~4QubTI", "parent_policy_id": "2024-05-18_02.04.36~E0JgPM", "score": 75.67693902250846, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:27:41.220682"}, "12": {"model_id": "2024-05-18_02.21.57~6nCQWI", "parent_model_id": "2024-05-18_02.10.22~OUv5D1", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.21.57~6nCQWI--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.21.57~6nCQWI", "parent_policy_id": "2024-05-18_02.10.22~OUv5D1", "score": 67.56977135851871, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:27:44.334151"}, "13": {"model_id": "2024-05-18_02.27.28~fUslnN", "parent_model_id": "2024-05-18_02.04.36~E0JgPM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.27.28~fUslnN--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.27.28~fUslnN", "parent_policy_id": "2024-05-18_02.04.36~E0JgPM", "score": 78.03428742486389, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:33:15.772439"}, "14": {"model_id": "2024-05-18_02.27.41~nDJMf2", "parent_model_id": "2024-05-18_02.04.36~iJLFQm", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.27.41~nDJMf2--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.27.41~nDJMf2", "parent_policy_id": "2024-05-18_02.04.36~iJLFQm", "score": 78.3870001952435, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:33:25.525780"}, "15": {"model_id": "2024-05-18_02.27.44~oYdZeY", "parent_model_id": "2024-05-18_02.04.36~E0JgPM", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.27.44~oYdZeY--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.27.44~oYdZeY", "parent_policy_id": "2024-05-18_02.04.36~E0JgPM", "score": 75.01822165260714, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:33:30.270166"}, "16": {"model_id": "2024-05-18_02.33.15~MSHPji", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.33.15~MSHPji--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.33.15~MSHPji", "parent_policy_id": null, "score": 71.76114458572842, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:38:54.989777"}, "17": {"model_id": "2024-05-18_02.33.25~w9PJcD", "parent_model_id": "2024-05-18_02.04.36~iJLFQm", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.33.25~w9PJcD--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.33.25~w9PJcD", "parent_policy_id": "2024-05-18_02.04.36~iJLFQm", "score": 79.29983926343111, "steps_trained": 200000, "env_steps_trained": 800000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:39:13.913768"}, "18": {"model_id": "2024-05-18_02.33.30~l99DVG", "parent_model_id": "2024-05-18_02.27.41~nDJMf2", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.33.30~l99DVG--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.33.30~l99DVG", "parent_policy_id": "2024-05-18_02.27.41~nDJMf2", "score": 80.3886173911618, "steps_trained": 300000, "env_steps_trained": 1200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:39:15.369094"}, "19": {"model_id": "2024-05-18_02.38.55~4NnEWS", "parent_model_id": "2024-05-18_02.27.41~nDJMf2", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.38.55~4NnEWS--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.38.55~4NnEWS", "parent_policy_id": "2024-05-18_02.27.41~nDJMf2", "score": 72.32185819225603, "steps_trained": 300000, "env_steps_trained": 1200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:44:39.406862"}, "20": {"model_id": "2024-05-18_02.39.15~HCRTWj", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.39.15~HCRTWj--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.39.15~HCRTWj", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 57.75688893046234, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:44:56.850738"}, "21": {"model_id": "2024-05-18_02.39.14~ZHvPBQ", "parent_model_id": "2024-05-18_02.33.25~w9PJcD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.39.14~ZHvPBQ--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.39.14~ZHvPBQ", "parent_policy_id": "2024-05-18_02.33.25~w9PJcD", "score": 77.4409790125244, "steps_trained": 300000, "env_steps_trained": 1200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:45:04.778607"}, "22": {"model_id": "2024-05-18_02.44.39~B62FhB", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.44.39~B62FhB--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.44.39~B62FhB", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 76.8289555608485, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:50:20.412800"}, "23": {"model_id": "2024-05-18_02.44.56~2C6LPQ", "parent_model_id": "2024-05-18_02.33.25~w9PJcD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.44.56~2C6LPQ--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.44.56~2C6LPQ", "parent_policy_id": "2024-05-18_02.33.25~w9PJcD", "score": 77.20214944273884, "steps_trained": 300000, "env_steps_trained": 1200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:50:45.972435"}, "24": {"model_id": "2024-05-18_02.45.04~rzZVFk", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.45.04~rzZVFk--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.45.04~rzZVFk", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 64.0752143520263, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:50:52.572254"}, "25": {"model_id": "2024-05-18_02.50.20~Tw80dJ", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.50.20~Tw80dJ--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.50.20~Tw80dJ", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 80.24982572804578, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:56:11.836560"}, "26": {"model_id": "2024-05-18_02.50.46~65wOhK", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.50.46~65wOhK--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.50.46~65wOhK", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 72.91700398795686, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:56:31.551052"}, "27": {"model_id": "2024-05-18_02.50.52~WfSPlL", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.50.52~WfSPlL--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.50.52~WfSPlL", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 82.6601984497056, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 02:56:34.784384"}, "28": {"model_id": "2024-05-18_02.56.11~X1wSM4", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.56.11~X1wSM4--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.56.11~X1wSM4", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 75.87453395290885, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:01:59.786413"}, "29": {"model_id": "2024-05-18_02.56.31~TjuLsK", "parent_model_id": "2024-05-18_02.33.30~l99DVG", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.56.31~TjuLsK--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.56.31~TjuLsK", "parent_policy_id": "2024-05-18_02.33.30~l99DVG", "score": 68.36496020478324, "steps_trained": 400000, "env_steps_trained": 1600000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:02:15.881366"}, "30": {"model_id": "2024-05-18_02.56.34~Pp2xzN", "parent_model_id": "2024-05-18_02.50.52~WfSPlL", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_02.56.34~Pp2xzN--state_dict.pth", "model_info": {"policy_id": "2024-05-18_02.56.34~Pp2xzN", "parent_policy_id": "2024-05-18_02.50.52~WfSPlL", "score": 12.653413355847096, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:02:19.310134"}, "31": {"model_id": "2024-05-18_03.01.59~PkOJwV", "parent_model_id": "2024-05-18_02.50.52~WfSPlL", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.01.59~PkOJwV--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.01.59~PkOJwV", "parent_policy_id": "2024-05-18_02.50.52~WfSPlL", "score": 19.734284940317664, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:07:47.059711"}, "32": {"model_id": "2024-05-18_03.02.16~Pu9yuF", "parent_model_id": "2024-05-18_02.50.52~WfSPlL", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.02.16~Pu9yuF--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.02.16~Pu9yuF", "parent_policy_id": "2024-05-18_02.50.52~WfSPlL", "score": 14.328596127334404, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:08:02.302650"}, "33": {"model_id": "2024-05-18_03.02.19~270wLU", "parent_model_id": "2024-05-18_02.33.25~w9PJcD", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.02.19~270wLU--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.02.19~270wLU", "parent_policy_id": "2024-05-18_02.33.25~w9PJcD", "score": 45.14784901377705, "steps_trained": 300000, "env_steps_trained": 1200000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:08:05.167930"}, "34": {"model_id": "2024-05-18_03.07.47~OJ2HR4", "parent_model_id": "2024-05-18_02.50.52~WfSPlL", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.07.47~OJ2HR4--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.07.47~OJ2HR4", "parent_policy_id": "2024-05-18_02.50.52~WfSPlL", "score": 5.677906639085142, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:13:32.385315"}, "35": {"model_id": "2024-05-18_03.08.05~SOsTBF", "parent_model_id": "2024-05-18_02.50.20~Tw80dJ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.08.05~SOsTBF--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.08.05~SOsTBF", "parent_policy_id": "2024-05-18_02.50.20~Tw80dJ", "score": 86.16689731148642, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:13:49.350732"}, "36": {"model_id": "2024-05-18_03.08.02~8t3C3G", "parent_model_id": "2024-05-18_02.50.20~Tw80dJ", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.08.02~8t3C3G--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.08.02~8t3C3G", "parent_policy_id": "2024-05-18_02.50.20~Tw80dJ", "score": 90.2009532526324, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:13:51.496868"}, "37": {"model_id": "2024-05-18_03.13.32~kIFvpY", "parent_model_id": "2024-05-18_02.50.52~WfSPlL", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.13.32~kIFvpY--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.13.32~kIFvpY", "parent_policy_id": "2024-05-18_02.50.52~WfSPlL", "score": 13.053932423151743, "steps_trained": 500000, "env_steps_trained": 2000000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:19:20.006232"}, "38": {"model_id": "2024-05-18_03.13.49~XXwWEt", "parent_model_id": "2024-05-18_03.08.05~SOsTBF", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.13.49~XXwWEt--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.13.49~XXwWEt", "parent_policy_id": "2024-05-18_03.08.05~SOsTBF", "score": 87.06851297503727, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:19:30.029707"}, "39": {"model_id": "2024-05-18_03.13.51~xypfM9", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.13.51~xypfM9--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.13.51~xypfM9", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 50.45442432770547, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:19:41.120332"}, "40": {"model_id": "2024-05-18_03.19.41~rIFzHm", "parent_model_id": null, "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.19.41~rIFzHm--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.19.41~rIFzHm", "parent_policy_id": null, "score": 59.09790209928117, "steps_trained": 100000, "env_steps_trained": 400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:25:12.021407"}, "41": {"model_id": "2024-05-18_03.19.20~RvRAwC", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.19.20~RvRAwC--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.19.20~RvRAwC", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 44.072630494491406, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:25:13.596031"}, "42": {"model_id": "2024-05-18_03.19.30~pZ1WoZ", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.19.30~pZ1WoZ--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.19.30~pZ1WoZ", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 60.93126608067904, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:25:19.859171"}, "43": {"model_id": "2024-05-18_03.25.13~q4HxG1", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.25.13~q4HxG1--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.25.13~q4HxG1", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 47.23458753529394, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:31:06.434216"}, "44": {"model_id": "2024-05-18_03.25.12~tX0ErA", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.25.12~tX0ErA--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.25.12~tX0ErA", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 45.053283837618366, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:31:06.858245"}, "45": {"model_id": "2024-05-18_03.25.19~z3A827", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.25.19~z3A827--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.25.19~z3A827", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 43.07660564868041, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:31:11.390311"}, "46": {"model_id": "2024-05-18_03.31.06~DNFOd9", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.31.06~DNFOd9--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.31.06~DNFOd9", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 59.10827861210164, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:36:59.473680"}, "47": {"model_id": "2024-05-18_03.31.07~SMvE6c", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.31.07~SMvE6c--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.31.07~SMvE6c", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 51.92181426431228, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:37:02.530035"}, "48": {"model_id": "2024-05-18_03.31.11~K0Yxfj", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.31.11~K0Yxfj--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.31.11~K0Yxfj", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 42.902266806863004, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:37:04.640336"}, "49": {"model_id": "2024-05-18_03.36.59~Twzw7c", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.36.59~Twzw7c--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.36.59~Twzw7c", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 66.87164926804732, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:42:48.682986"}, "50": {"model_id": "2024-05-18_03.37.02~ZkZ45M", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.37.02~ZkZ45M--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.37.02~ZkZ45M", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 43.27195331788608, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:42:56.072349"}, "51": {"model_id": "2024-05-18_03.37.04~ciLtj8", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.37.04~ciLtj8--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.37.04~ciLtj8", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 47.845121555412064, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:42:58.720141"}, "52": {"model_id": "2024-05-18_03.42.48~MbgdKI", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.42.48~MbgdKI--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.42.48~MbgdKI", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 47.715984631303044, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:48:42.947188"}, "53": {"model_id": "2024-05-18_03.42.56~0oOWmB", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.42.56~0oOWmB--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.42.56~0oOWmB", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 59.871357037745135, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:48:45.522764"}, "54": {"model_id": "2024-05-18_03.42.58~i5LtDv", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.42.58~i5LtDv--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.42.58~i5LtDv", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 45.058117910496605, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:48:52.311006"}, "55": {"model_id": "2024-05-18_03.48.43~D5Dyl0", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.48.43~D5Dyl0--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.48.43~D5Dyl0", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 64.5842565702098, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:54:35.910779"}, "56": {"model_id": "2024-05-18_03.48.45~AX9jJl", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.48.45~AX9jJl--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.48.45~AX9jJl", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 43.951509823218245, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:54:37.552474"}, "57": {"model_id": "2024-05-18_03.48.52~cI3Yq2", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.48.52~cI3Yq2--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.48.52~cI3Yq2", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 44.3267087948441, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 03:54:45.877373"}, "58": {"model_id": "2024-05-18_03.54.36~12xDiB", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.54.36~12xDiB--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.54.36~12xDiB", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 44.90346084057597, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 04:00:31.000339"}, "59": {"model_id": "2024-05-18_03.54.37~o74NQW", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.54.37~o74NQW--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.54.37~o74NQW", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 43.46528815156293, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 04:00:31.560464"}, "60": {"model_id": "2024-05-18_03.54.46~S01oBe", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_03.54.46~S01oBe--state_dict.pth", "model_info": {"policy_id": "2024-05-18_03.54.46~S01oBe", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 45.39761152051865, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 04:00:38.411360"}, "61": {"model_id": "2024-05-18_04.00.31~V3rb0l", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_04.00.31~V3rb0l--state_dict.pth", "model_info": {"policy_id": "2024-05-18_04.00.31~V3rb0l", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 44.28031892847575, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 04:06:24.728792"}, "62": {"model_id": "2024-05-18_04.00.31~uj5kRQ", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_04.00.31~uj5kRQ--state_dict.pth", "model_info": {"policy_id": "2024-05-18_04.00.31~uj5kRQ", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 57.00997456102605, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 04:06:24.910832"}, "63": {"model_id": "2024-05-18_04.00.38~0vNqJh", "parent_model_id": "2024-05-18_03.08.02~8t3C3G", "state_dict_path": "E:/saved_models/rl/Humanoid-v4/mitosis-2024-05-18_02.04.34\\2024-05-18_04.00.38~0vNqJh--state_dict.pth", "model_info": {"policy_id": "2024-05-18_04.00.38~0vNqJh", "parent_policy_id": "2024-05-18_03.08.02~8t3C3G", "score": 43.14194475866027, "steps_trained": 600000, "env_steps_trained": 2400000, "init_policy_source_code": "def init_policy():\n    import torch\n    from torch import nn\n\n    from src.networks.core.seq_net import SeqNet\n    from src.reinforcement_learning.core.action_selectors.squashed_diag_gaussian_action_selector import \\\n        SquashedDiagGaussianActionSelector\n    from src.reinforcement_learning.core.policies.actor_critic_policy import ActorCriticPolicy\n\n    in_size = 376\n    action_size = 17\n    actor_out_sizes = [512, 512, 256, 256, 256, 256, 256, 256]\n    critic_out_sizes = [512, 512, 256, 256, 256, 1]\n\n    hidden_activation_function = nn.ELU()\n\n    class A2CNetwork(nn.Module):\n\n        def __init__(self):\n            super().__init__()\n\n\n            self.actor = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=actor_out_sizes\n            )\n\n            self.critic = SeqNet.from_layer_provider(\n                layer_provider=lambda layer_nr, is_last_layer, in_features, out_features: nn.Sequential(\n                    nn.Linear(in_features, out_features),\n                    nn.Identity() if is_last_layer else hidden_activation_function\n                ),\n                in_size=in_size,\n                out_sizes=critic_out_sizes\n            )\n\n        def forward(self, x: torch.Tensor):\n            return self.actor(x), self.critic(x)\n\n    return ActorCriticPolicy(A2CNetwork(), SquashedDiagGaussianActionSelector(\n        latent_dim=actor_out_sizes[-1],\n        action_dim=action_size,\n        std=0.1,\n        std_learnable=False,\n    ))\n", "wrap_env_source_code": "def wrap_env(_env):\n    from src.reinforcement_learning.gym.envs.transform_reward_wrapper import TransformRewardWrapper\n    from gymnasium.wrappers import RescaleAction\n\n    _env = TransformRewardWrapper(_env, lambda _reward: 0.01 * _reward)\n    _env = RescaleAction(_env, min_action=-1.0, max_action=1.0)\n\n    return _env\n"}, "last_update_time": "2024-05-18 04:06:31.583744"}}}